{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W_jOQpxq-fhf",
    "outputId": "14aa3ed7-4ae0-4249-c5b8-80d293c78211"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DNcNC7J8-jMw"
   },
   "outputs": [],
   "source": [
    "# All general imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, Reshape, Conv2D, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Bidirectional, GlobalAveragePooling1D, GRU, GlobalMaxPooling1D, concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LSTM, GRU, Conv1D, MaxPool1D, Activation\n",
    "from keras.layers import add\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Conv1D, Softmax\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import io, os, gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "wsfH1aDj-l0X",
    "outputId": "ba4dd793-e52f-461d-ab19-544ca1525db4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Headline', 'Body ID', 'Stance', 'Body', 'Novelty_Labels', 'Emotion_1'], dtype='object')\n",
      "Index(['Headline', 'Body ID', 'Stance', 'Body', 'Novelty_Labels', 'Emotion_1'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body ID</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Body</th>\n",
       "      <th>Novelty_Labels</th>\n",
       "      <th>Emotion_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ferguson riots: Pregnant woman loses eye after...</td>\n",
       "      <td>2008</td>\n",
       "      <td>3</td>\n",
       "      <td>A RESPECTED senior French police officer inves...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crazy Conservatives Are Sure a Gitmo Detainee ...</td>\n",
       "      <td>1550</td>\n",
       "      <td>3</td>\n",
       "      <td>Dave Morin's social networking company Path is...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Russian Guy Says His Justin Bieber Ringtone ...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>A bereaved Afghan mother took revenge on the T...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zombie Cat: Buried Kitty Believed Dead, Meows ...</td>\n",
       "      <td>1793</td>\n",
       "      <td>3</td>\n",
       "      <td>Hewlett-Packard is officially splitting in two...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Argentina's President Adopts Boy to End Werewo...</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>An airline passenger headed to Dallas was remo...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>turst</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  ...  Emotion_1\n",
       "0  Ferguson riots: Pregnant woman loses eye after...  ...      anger\n",
       "1  Crazy Conservatives Are Sure a Gitmo Detainee ...  ...       fear\n",
       "2  A Russian Guy Says His Justin Bieber Ringtone ...  ...        joy\n",
       "3  Zombie Cat: Buried Kitty Believed Dead, Meows ...  ...        joy\n",
       "4  Argentina's President Adopts Boy to End Werewo...  ...      turst\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################### Importing FNC Datasets ####################\n",
    "# Train set\n",
    "train_df = pd.read_csv('../Datasets/train_fnc.csv')\n",
    "print(train_df.columns)\n",
    "le = LabelEncoder()\n",
    "train_df['Stance'] = le.fit_transform(train_df['Stance'])\n",
    "train_df.head()\n",
    "\n",
    "# Test set\n",
    "test_df = pd.read_csv('../Datasets/test_fnc.csv')\n",
    "print(test_df.columns)\n",
    "test_df['Stance'] = le.transform(test_df['Stance'])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hCenMCQA_SNx",
    "outputId": "89615ba8-8de0-4547-cc28-cffefa2a3e0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise (4518, 768)\n",
      "Hypothesis (4518, 768)\n"
     ]
    }
   ],
   "source": [
    "# Getting BERT Embeddings (only for agreed and disagree class)\n",
    "pre_bert_fnc = np.load(\"../Datasets/pre_bert_fnc.npy\")\n",
    "hyp_bert_fnc = np.load(\"../Datasets/hyp_bert_fnc.npy\")\n",
    "print('Premise', pre_bert_fnc.shape)\n",
    "print('Hypothesis', hyp_bert_fnc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w1mCT23I_rrB",
    "outputId": "aea66dad-a7ac-4ac5-93c6-1fd8298f6ec7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise (2600, 768)\n",
      "Hypothesis (2600, 768)\n"
     ]
    }
   ],
   "source": [
    "# Bert embeddings for test\n",
    "pre_bert_fnc_test = np.load(\"../Datasets/pre_bert_test_fnc.npy\")\n",
    "hyp_bert_fnc_test = np.load(\"../Datasets/hyp_bert_test_fnc.npy\")\n",
    "print('Premise', pre_bert_fnc_test.shape)\n",
    "print('Hypothesis', hyp_bert_fnc_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PWt94VD8-oKx",
    "outputId": "44961b6c-9eef-4dea-cc9f-d08b1c0a26cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49972\n",
      "49972\n",
      "1669\n",
      "1648\n",
      "Train Length is 3317\n",
      "Test merged 1794\n",
      "Dataset length is 5111\n"
     ]
    }
   ],
   "source": [
    "train_lst_1 = train_df['Body'].tolist()\n",
    "print(len(train_lst_1))\n",
    "train_lst_1[:5]\n",
    "train_lst_2 = train_df['Headline'].tolist()\n",
    "print(len(train_lst_2))\n",
    "uq_tr_1 = list(set(train_lst_1))\n",
    "uq_tr_2 = list(set(train_lst_2))\n",
    "print(len(uq_tr_1))\n",
    "print(len(uq_tr_2))\n",
    "train_merged = uq_tr_1 + uq_tr_2\n",
    "print('Train Length is', len(train_merged))\n",
    "train_merged[:5]\n",
    "test_lst_1 = test_df['Body'].tolist()\n",
    "test_lst_2 = test_df['Headline'].tolist()\n",
    "uq_ts_1 = list(set(test_lst_1))\n",
    "uq_ts_2 = list(set(test_lst_2))\n",
    "test_merged = uq_ts_1 + uq_ts_2\n",
    "print('Test merged', len(test_merged))\n",
    "total_dataset = train_merged + test_merged\n",
    "print('Dataset length is', len(total_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4YDAfOtB-yo3"
   },
   "outputs": [],
   "source": [
    "# Defining the tokenizer\n",
    "def get_tokenizer(vocabulary_size):\n",
    "  print('Training tokenizer...')\n",
    "  tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "  tweet_text = []\n",
    "  print('Read {} Sentences'.format(len(total_dataset)))\n",
    "  tokenizer.fit_on_texts(total_dataset)\n",
    "  return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9vimr9k8-05J"
   },
   "outputs": [],
   "source": [
    "# For getting the embedding matrix\n",
    "def get_embeddings():\n",
    "  print('Generating embeddings matrix...')\n",
    "  embeddings_file = 'glove.6B.300d.txt'\n",
    "  embeddings_index = dict()\n",
    "  with open(embeddings_file, 'r', encoding=\"utf-8\") as infile:\n",
    "    for line in infile:\n",
    "      values = line.split()\n",
    "      word = values[0]\n",
    "      vector = np.asarray(values[1:], \"float32\")\n",
    "      embeddings_index[word] = vector\n",
    "\t# create a weight matrix for words in training docs\n",
    "  vocabulary_size = len(embeddings_index)\n",
    "  embeddinds_size = list(embeddings_index.values())[0].shape[0]\n",
    "  print('Vocabulary = {}, embeddings = {}'.format(vocabulary_size, embeddinds_size))\n",
    "  tokenizer = get_tokenizer(vocabulary_size)\n",
    "  embedding_matrix = np.zeros((vocabulary_size, embeddinds_size))\n",
    "  considered = 0\n",
    "  total = len(tokenizer.word_index.items())\n",
    "  for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "      print(word, index)\n",
    "      continue\n",
    "    else:\n",
    "      embedding_vector = embeddings_index.get(word)\n",
    "      if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "        considered += 1\n",
    "  print('Considered ', considered, 'Left ', total - considered)\t\t\t\n",
    "  return embedding_matrix, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "QXxVXVYq-3EC"
   },
   "outputs": [],
   "source": [
    "def get_data(tokenizer, MAX_LENGTH, input_df):\n",
    "  print('Loading data')\n",
    "  X1, X2, Y = [], [], []\n",
    "  X1 = input_df['Body'].tolist()\n",
    "  X2 = input_df['Headline'].tolist()\n",
    "  Y = input_df['Stance'].tolist()\n",
    "  \n",
    "  assert len(X1) == len(X2) == len(Y)\n",
    "  sequences_1 = tokenizer.texts_to_sequences(X1)\n",
    "  sequences_2 = tokenizer.texts_to_sequences(X2)\n",
    "  X1 = pad_sequences(sequences_1, maxlen=MAX_LENGTH)\n",
    "  X2 = pad_sequences(sequences_2, maxlen=MAX_LENGTH)\n",
    "  Y = np.array(Y)\n",
    "  return X1, X2, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n2NEYvKm-43L",
    "outputId": "8cfaa3e3-7a66-492c-dad6-0811c101ebe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings matrix...\n",
      "Vocabulary = 400000, embeddings = 300\n",
      "Training tokenizer...\n",
      "Read 5111 Sentences\n",
      "Considered  26192 Left  11274\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, tokenizer = get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r74oJfGG-6Mg",
    "outputId": "caf87ea8-f00b-40ce-babb-df02cc14e9af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 100\n",
    "# read ml data\n",
    "X1, X2, Y = get_data(tokenizer, MAX_LENGTH, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D21413B1-7Y4",
    "outputId": "93c1236e-0846-40c1-97e5-8ff023045314"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "X1_test, X2_test, Y_test = get_data(tokenizer, MAX_LENGTH, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-lhaiTXQ-8eZ",
    "outputId": "cc12eee1-039f-42a0-fffa-15f671ccd580"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49972,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(49972, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Y.shape)\n",
    "print(type(X1))\n",
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3_8hfJHS_AOY",
    "outputId": "dc04f822-249f-4204-f440-ee60242a0c9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8909,) (36545,)\n",
      "(45454,)\n",
      "Train shape (4518, 100)\n",
      "Train labels [0 1 0 ... 0 1 0]\n",
      "Test shape (2600, 100)\n",
      "Test labels [0 0 0 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Removing the unrelated samples from both train and test\n",
    "result = np.where(train_df['Stance'] == 2)[0]\n",
    "result_1 = np.where(train_df['Stance'] == 3)[0]\n",
    "print(result.shape, result_1.shape)\n",
    "result_comb = np.concatenate((result, result_1))\n",
    "print(result_comb.shape)\n",
    "reduced_X1 = np.delete(X1, result_comb, axis=0)\n",
    "reduced_X2 = np.delete(X2, result_comb, axis=0)\n",
    "print('Train shape', reduced_X1.shape)\n",
    "reduced_train_labels = np.delete(train_df['Stance'].values, result_comb)\n",
    "print('Train labels', reduced_train_labels)\n",
    "result_test = np.where(test_df['Stance'] == 2)[0]\n",
    "result_test_1 = np.where(test_df['Stance'] == 3)[0]\n",
    "result_test_comb = np.concatenate((result_test, result_test_1))\n",
    "reduced_X1_test = np.delete(X1_test, result_test_comb, axis=0)\n",
    "reduced_X2_test = np.delete(X2_test, result_test_comb, axis=0)\n",
    "print('Test shape', reduced_X1_test.shape)\n",
    "reduced_test_labels = np.delete(test_df['Stance'].values, result_test_comb)\n",
    "print('Test labels', reduced_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "sIyHTNjm_Bo6"
   },
   "outputs": [],
   "source": [
    "embeddings_file = 'glove.6B.300d.txt'\n",
    "embeddings_index = dict()\n",
    "with open(embeddings_file, 'r', encoding=\"utf-8\") as infile:\n",
    "  for line in infile:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:], \"float32\")\n",
    "    embeddings_index[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SIMa10i4_C9w",
    "outputId": "5df1edf4-afb8-4fd7-9485-71506b6996d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "y_train = keras.utils.to_categorical(reduced_train_labels)\n",
    "print(y_train)\n",
    "y_test = keras.utils.to_categorical(reduced_test_labels)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "rGjfGHLv_Pob"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "VALIDATION_RATIO = 0.1\n",
    "RANDOM_STATE = 9527\n",
    "x1_train, x1_val, \\\n",
    "x2_train, x2_val, \\\n",
    "pre_train_bert, pre_val_bert, \\\n",
    "hyp_train_bert, hyp_val_bert, \\\n",
    "y_train, y_val = \\\n",
    "    train_test_split(\n",
    "        reduced_X1, reduced_X2, pre_bert_fnc, hyp_bert_fnc, y_train, \n",
    "        test_size=VALIDATION_RATIO, \n",
    "        random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_6coYfiHApQ5",
    "outputId": "87383bef-a9e3-4f55-c2c5-05b82f5928a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "----------\n",
      "x1_train: (4066, 100)\n",
      "x2_train: (4066, 100)\n",
      "y_train_cs : (4066, 2)\n",
      "Bert premise : (4066, 768)\n",
      "----------\n",
      "x1_val:   (452, 100)\n",
      "x2_val:   (452, 100)\n",
      "y_val_cs :   (452, 2)\n",
      "Val_Bert_premise : (452, 768)\n",
      "----------\n",
      "Test Set\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set\")\n",
    "print(\"-\" * 10)\n",
    "print(f\"x1_train: {x1_train.shape}\")\n",
    "print(f\"x2_train: {x2_train.shape}\")\n",
    "print(f\"y_train : {y_train.shape}\")\n",
    "print(f\"Bert premise : {pre_train_bert.shape}\")\n",
    "\n",
    "print(\"-\" * 10)\n",
    "print(f\"x1_val:   {x1_val.shape}\")\n",
    "print(f\"x2_val:   {x2_val.shape}\")\n",
    "print(f\"y_val :   {y_val.shape}\")\n",
    "print(f\"Val_Bert_premise : {pre_val_bert.shape}\")\n",
    "print(\"-\" * 10)\n",
    "print(\"Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "UXWrl7KkBd8c"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "NUM_LSTM_UNITS = 150\n",
    "\n",
    "MAX_NUM_WORDS = embedding_matrix.shape[0]\n",
    "\n",
    "NUM_EMBEDDING_DIM = embedding_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KKP12wuABh54",
    "outputId": "2ba54214-d276-4cd0-f7c9-bd67701c9420"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 768)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 768)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 100, 300)     120000000   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 1, 768)       0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 768)       0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 100, 300)     541200      embedding[0][0]                  \n",
      "                                                                 embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1, 300)       1102800     reshape[0][0]                    \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 100, 600)     0           bidirectional[0][0]              \n",
      "                                                                 bidirectional[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 600)       0           bidirectional_2[0][0]            \n",
      "                                                                 bidirectional_2[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 300)          901200      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 300)          901200      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2 (TensorFlowOp [(None, 300)]        0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub (TensorFlowOpLa [(None, 300)]        0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul (TensorFlowOpLa [(None, 300)]        0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 900)          0           tf_op_layer_AddV2[0][0]          \n",
      "                                                                 tf_op_layer_Sub[0][0]            \n",
      "                                                                 tf_op_layer_Mul[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "pre_fnc (Dense)                 (None, 64)           57664       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "fnc (Dense)                     (None, 2)            130         pre_fnc[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 123,504,194\n",
      "Trainable params: 123,504,194\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# BERT + Normal Grand Model\n",
    "\n",
    "NUM_LSTM_UNITS = 150\n",
    "\n",
    "top_input_wd = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "    dtype='int32')\n",
    "bm_input_wd = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "    dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
    "top_embedded_wd = embedding_layer(\n",
    "    top_input_wd)\n",
    "bm_embedded_wd = embedding_layer(\n",
    "    bm_input_wd)\n",
    "\n",
    "source_lstm_wd = Bidirectional(LSTM(NUM_LSTM_UNITS, return_sequences=True, recurrent_dropout = 0.3))\n",
    "shared_lstm_wd = Bidirectional(LSTM(NUM_LSTM_UNITS, activation='tanh', recurrent_dropout = 0.3))\n",
    "top_source_wd = source_lstm_wd(top_embedded_wd)\n",
    "bm_source_wd = source_lstm_wd(bm_embedded_wd)\n",
    "\n",
    "source_comb_wd = concatenate(\n",
    "    [top_source_wd, bm_source_wd],\n",
    "    axis=-1\n",
    "    )\n",
    "lstm_ops_wd = shared_lstm_wd(source_comb_wd)   # 300D vector\n",
    "\n",
    "\n",
    "top_input_bt = Input(\n",
    "    shape=(768, ), \n",
    "    dtype='float32')\n",
    "bm_input_bt = Input(\n",
    "    shape=(768, ), \n",
    "    dtype='float32')\n",
    "\n",
    "\n",
    "top_embedded_bt = Reshape((1, 768, ))(top_input_bt)\n",
    "bm_embedded_bt = Reshape((1, 768, ))(bm_input_bt)\n",
    "\n",
    "source_lstm_bt = Bidirectional(LSTM(NUM_LSTM_UNITS, return_sequences=True, recurrent_dropout = 0.3))\n",
    "shared_lstm_bt = Bidirectional(LSTM(NUM_LSTM_UNITS, activation='tanh', recurrent_dropout = 0.3))\n",
    "top_source_bt = source_lstm_bt(top_embedded_bt)\n",
    "bm_source_bt = source_lstm_bt(bm_embedded_bt)\n",
    "\n",
    "source_comb_bt = concatenate(\n",
    "    [top_source_bt, bm_source_bt],\n",
    "    axis=-1\n",
    "    )\n",
    "lstm_ops_bt = shared_lstm_bt(source_comb_bt)  #300D vector\n",
    "\n",
    "# Bert and Normal Combination\n",
    "\n",
    "comb_features_fnc = concatenate(\n",
    "    [lstm_ops_wd+lstm_ops_bt, lstm_ops_wd-lstm_ops_bt, lstm_ops_wd*lstm_ops_bt],\n",
    "    axis=-1\n",
    "    )\n",
    "\n",
    "pre_fnc = Dense(\n",
    "    units=64, \n",
    "    activation='tanh',\n",
    "    name = 'pre_fnc')(comb_features_fnc)\n",
    "\n",
    "dense_fnc =  Dense(\n",
    "    units=NUM_CLASSES, \n",
    "    activation='softmax',\n",
    "    name = 'fnc')\n",
    "\n",
    "predictions_fnc = dense_fnc(pre_fnc)\n",
    "\n",
    "model = Model(\n",
    "    inputs=[top_input_wd, bm_input_wd, top_input_bt, bm_input_bt], \n",
    "    outputs=[predictions_fnc])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "4PqdrT4gCSfM"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "lr = 1e-3\n",
    "opt = Adam(lr=lr, decay=lr/50)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "aa2AHqtECc-F"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weight = class_weight.compute_class_weight('balanced'\n",
    "                                               ,np.unique(reduced_train_labels)\n",
    "                                               ,reduced_train_labels)\n",
    "class_weight_dict = dict(enumerate(class_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xE0Hcxd4Cf18",
    "outputId": "a47648ea-2ea7-47df-853d-18a580d1c1db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 194s 12s/step - loss: 0.6400 - accuracy: 0.6481 - val_loss: 0.3573 - val_accuracy: 0.8208\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 188s 12s/step - loss: 0.3420 - accuracy: 0.8320 - val_loss: 0.4186 - val_accuracy: 0.8518\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 10\n",
    "stop = [EarlyStopping(monitor='val_loss', patience=0.001)]\n",
    "history = model.fit(x=[x1_train, x2_train, pre_train_bert, hyp_train_bert],\n",
    "                    y=y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=(\n",
    "                      [x1_val, x2_val, pre_val_bert, hyp_val_bert], \n",
    "                      y_val\n",
    "                    ),\n",
    "                    shuffle=True,\n",
    "                    callbacks=stop,\n",
    "                    class_weight = class_weight_dict\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "OfGos7FIEjNQ"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "predictions = model.predict(\n",
    "    [reduced_X1_test, reduced_X2_test, pre_bert_fnc_test, hyp_bert_fnc_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QHcK6i5NEv0b",
    "outputId": "3a7754ea-16a0-47db-db02-3b0f6e84007e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is\n",
      "68.15384615384616\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      agreed       0.74      0.86      0.80      1903\n",
      "   disagreed       0.33      0.19      0.24       697\n",
      "\n",
      "    accuracy                           0.68      2600\n",
      "   macro avg       0.54      0.53      0.52      2600\n",
      "weighted avg       0.63      0.68      0.65      2600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = [idx for idx in np.argmax(predictions, axis=1)]\n",
    "print('Accuracy is')\n",
    "print(metrics.accuracy_score(reduced_test_labels, y_pred)*100)\n",
    "print(classification_report(reduced_test_labels, y_pred, target_names = ['agreed', 'disagreed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jNx0Ar7oFmLT",
    "outputId": "3d3d5407-d92d-411d-d503-e15e732a16eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Text CNN model...\n",
      "Model: \"functional_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 100, 300)     120000000   input_13[0][0]                   \n",
      "                                                                 input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)            (None, 100, 300, 1)  0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_11 (Reshape)            (None, 100, 300, 1)  0           embedding_4[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 99, 1, 128)   76928       reshape_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 98, 1, 128)   115328      reshape_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           [(None, 768)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           [(None, 768)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 1, 1, 128)    0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 1, 1, 128)    0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_12 (Reshape)            (None, 1, 768)       0           input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)            (None, 1, 768)       0           input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 2, 1, 128)    0           max_pooling2d_6[0][0]            \n",
      "                                                                 max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 1, 256)       918528      reshape_12[0][0]                 \n",
      "                                                                 reshape_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 256)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 1, 512)       0           bidirectional_6[0][0]            \n",
      "                                                                 bidirectional_6[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 256)          0           flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 256)          656384      concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_2 (TensorFlow [(None, 256)]        0           dropout_3[0][0]                  \n",
      "                                                                 bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_2 (TensorFlowOp [(None, 256)]        0           dropout_3[0][0]                  \n",
      "                                                                 bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_2 (TensorFlowOp [(None, 256)]        0           dropout_3[0][0]                  \n",
      "                                                                 bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 768)          0           tf_op_layer_AddV2_2[0][0]        \n",
      "                                                                 tf_op_layer_Sub_2[0][0]          \n",
      "                                                                 tf_op_layer_Mul_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            1538        concatenate_10[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 121,768,706\n",
      "Trainable params: 121,768,706\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model 2\n",
    "NUM_LSTM_UNITS = 128\n",
    "\n",
    "print('Getting Text CNN model...')\n",
    "filter_sizes = [2, 3, 5]\n",
    "num_filters = 128\t#Hyperparameters 32,64,128; 0.2,0.3,0.4\n",
    "drop = 0.4\n",
    "\n",
    "top_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "    dtype='int32')\n",
    "bm_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "    dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
    "top_embedded = embedding_layer(\n",
    "    top_input)\n",
    "bm_embedded = embedding_layer(\n",
    "    bm_input)\n",
    "reshape = Reshape((MAX_SEQUENCE_LENGTH, NUM_EMBEDDING_DIM, 1))(top_embedded)\n",
    "reshape_1 = Reshape((MAX_SEQUENCE_LENGTH, NUM_EMBEDDING_DIM, 1))(bm_embedded)\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], NUM_EMBEDDING_DIM),  padding='valid', kernel_initializer='normal',  activation='relu')(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], NUM_EMBEDDING_DIM),  padding='valid', kernel_initializer='normal',  activation='relu')(reshape_1)\n",
    "maxpool_0 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "\n",
    "top_input_bt = Input(\n",
    "    shape=(768, ), \n",
    "    dtype='float32')\n",
    "bm_input_bt = Input(\n",
    "    shape=(768, ), \n",
    "    dtype='float32')\n",
    "\n",
    "\n",
    "top_embedded_bt = Reshape((1, 768, ))(top_input_bt)\n",
    "bm_embedded_bt = Reshape((1, 768, ))(bm_input_bt)\n",
    "\n",
    "source_lstm_bt = Bidirectional(LSTM(NUM_LSTM_UNITS, return_sequences=True, recurrent_dropout = 0.3))\n",
    "shared_lstm_bt = Bidirectional(LSTM(NUM_LSTM_UNITS, activation='tanh', recurrent_dropout = 0.3))\n",
    "top_source_bt = source_lstm_bt(top_embedded_bt)\n",
    "bm_source_bt = source_lstm_bt(bm_embedded_bt)\n",
    "\n",
    "source_comb_bt = concatenate(\n",
    "    [top_source_bt, bm_source_bt],\n",
    "    axis=-1\n",
    "    )\n",
    "lstm_ops_bt = shared_lstm_bt(source_comb_bt)  #256D vector\n",
    "\n",
    "comb_features_fnc = concatenate(\n",
    "    [dropout+lstm_ops_bt, dropout-lstm_ops_bt, dropout*lstm_ops_bt],\n",
    "    axis=-1\n",
    "    )\n",
    "\n",
    "predictions = Dense(units=NUM_CLASSES, activation='sigmoid')(comb_features_fnc)\n",
    "\n",
    "model = Model(\n",
    "    inputs=[top_input, bm_input, top_input_bt, bm_input_bt], \n",
    "    outputs=predictions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "XYA11uE4Hqv8"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "lr = 1e-3\n",
    "opt = Adam(lr=lr, decay=lr/50)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A0dJnjeSHubA",
    "outputId": "bb5d2834-43f0-438e-8264-99d16a7721ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 35s 2s/step - loss: 0.6701 - accuracy: 0.5482 - val_loss: 0.5444 - val_accuracy: 0.8296\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 33s 2s/step - loss: 0.5026 - accuracy: 0.8079 - val_loss: 0.3978 - val_accuracy: 0.8053\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 33s 2s/step - loss: 0.3387 - accuracy: 0.8414 - val_loss: 0.4161 - val_accuracy: 0.7965\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 10\n",
    "stop = [EarlyStopping(monitor='val_loss', patience=0.001)]\n",
    "history = model.fit(x=[x1_train, x2_train, pre_train_bert, hyp_train_bert],\n",
    "                    y=y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=(\n",
    "                      [x1_val, x2_val, pre_val_bert, hyp_val_bert], \n",
    "                      y_val\n",
    "                    ),\n",
    "                    shuffle=True,\n",
    "                    callbacks=stop,\n",
    "                    class_weight = class_weight_dict\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "wMqYi5f_HxUy"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "predictions = model.predict(\n",
    "    [reduced_X1_test, reduced_X2_test, pre_bert_fnc_test, hyp_bert_fnc_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h6F8MS9rH5UZ",
    "outputId": "49c08975-4b69-4753-9fb9-dfbc6b3ad4a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is\n",
      "72.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      agreed       0.76      0.92      0.83      1903\n",
      "   disagreed       0.47      0.19      0.27       697\n",
      "\n",
      "    accuracy                           0.73      2600\n",
      "   macro avg       0.61      0.55      0.55      2600\n",
      "weighted avg       0.68      0.72      0.68      2600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = [idx for idx in np.argmax(predictions, axis=1)]\n",
    "print('Accuracy is')\n",
    "print(metrics.accuracy_score(reduced_test_labels, y_pred)*100)\n",
    "print(classification_report(reduced_test_labels, y_pred, target_names = ['agreed', 'disagreed']))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Fnc_Bert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
