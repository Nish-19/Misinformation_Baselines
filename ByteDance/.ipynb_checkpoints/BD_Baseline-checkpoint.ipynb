{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mv53mLhtfbA6",
    "outputId": "932a8a77-4ebc-4eca-d176-0b8fb7155ff7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yVb9M-KtfeH6"
   },
   "outputs": [],
   "source": [
    "# All general imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, Reshape, Conv2D, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Bidirectional, GlobalAveragePooling1D, GRU, GlobalMaxPooling1D, concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LSTM, GRU, Conv1D, MaxPool1D, Activation\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Conv1D, Softmax\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import io, os, gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "gKTq7DodfjWL",
    "outputId": "8158ba7b-0e1d-4720-f776-71490309827f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'tid1', 'tid2', 'title1_zh', 'title2_zh', 'title1_en',\n",
      "       'title2_en', 'label'],\n",
      "      dtype='object')\n",
      "Index(['id', 'tid1', 'tid2', 'title1_zh', 'title2_zh', 'title1_en',\n",
      "       'title2_en', 'Expected', 'Weight', 'Usage'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tid1</th>\n",
       "      <th>tid2</th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>title1_en</th>\n",
       "      <th>title2_en</th>\n",
       "      <th>Expected</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>321187</td>\n",
       "      <td>167562</td>\n",
       "      <td>59521</td>\n",
       "      <td>萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大</td>\n",
       "      <td>辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？</td>\n",
       "      <td>egypt 's presidential election failed to win m...</td>\n",
       "      <td>Lyon! Lyon officials have denied that Felipe F...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>Private</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321190</td>\n",
       "      <td>167564</td>\n",
       "      <td>91315</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国</td>\n",
       "      <td>A message from Saddam Hussein after he was cap...</td>\n",
       "      <td>The Top 10 Americans believe that the Lizard M...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>Public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>321189</td>\n",
       "      <td>167563</td>\n",
       "      <td>167564</td>\n",
       "      <td>萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>Will the United States wage war on Iraq withou...</td>\n",
       "      <td>A message from Saddam Hussein after he was cap...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>Private</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>321193</td>\n",
       "      <td>167564</td>\n",
       "      <td>160994</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>被绞刑处死的萨达姆是替身？他的此男人举动击破替身谣言！</td>\n",
       "      <td>A message from Saddam Hussein after he was cap...</td>\n",
       "      <td>The hanging Saddam is a surrogate? This man's ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>Public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321191</td>\n",
       "      <td>167564</td>\n",
       "      <td>15084</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>中国川贝枇杷膏在美国受到热捧？纯属谣言！</td>\n",
       "      <td>A message from Saddam Hussein after he was cap...</td>\n",
       "      <td>Chinese loquat loquat plaster in America? Pure...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>Public</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    tid1    tid2  ... Expected  Weight    Usage\n",
       "0  321187  167562   59521  ...        2  0.0625  Private\n",
       "1  321190  167564   91315  ...        2  0.0625   Public\n",
       "2  321189  167563  167564  ...        2  0.0625  Private\n",
       "3  321193  167564  160994  ...        2  0.0625   Public\n",
       "4  321191  167564   15084  ...        2  0.0625   Public\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################### Importing ByteDance Datasets ####################\n",
    "# Train set\n",
    "train_df = pd.read_csv('train_bd.csv')\n",
    "print(train_df.columns)\n",
    "le = LabelEncoder()\n",
    "train_df['label'] = le.fit_transform(train_df['label'])\n",
    "train_df.head()\n",
    "\n",
    "# Test set\n",
    "test_df = pd.read_csv('test_bd.csv')\n",
    "print(test_df.columns)\n",
    "test_df['Expected'] = le.transform(test_df['Expected'])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2mt6r5bBmiWA",
    "outputId": "bc8182a0-1635-4bb7-c656-b5f2f3ea1c7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320552\n",
      "320552\n",
      "67869\n",
      "136111\n",
      "Train Length is 203980\n",
      "Test merged 68725\n",
      "Dataset length is 272705\n"
     ]
    }
   ],
   "source": [
    "train_lst_1 = train_df['title1_en'].tolist()\n",
    "print(len(train_lst_1))\n",
    "train_lst_1[:5]\n",
    "train_lst_2 = train_df['title2_en'].tolist()\n",
    "print(len(train_lst_2))\n",
    "uq_tr_1 = list(set(train_lst_1))\n",
    "uq_tr_2 = list(set(train_lst_2))\n",
    "print(len(uq_tr_1))\n",
    "print(len(uq_tr_2))\n",
    "train_merged = uq_tr_1 + uq_tr_2\n",
    "print('Train Length is', len(train_merged))\n",
    "train_merged[:5]\n",
    "test_lst_1 = test_df['title1_en'].tolist()\n",
    "test_lst_2 = test_df['title2_en'].tolist()\n",
    "uq_ts_1 = list(set(test_lst_1))\n",
    "uq_ts_2 = list(set(test_lst_2))\n",
    "test_merged = uq_ts_1 + uq_ts_2\n",
    "print('Test merged', len(test_merged))\n",
    "total_dataset = train_merged + test_merged\n",
    "print('Dataset length is', len(total_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fZdPhwWrhdLG"
   },
   "outputs": [],
   "source": [
    "# Defining the tokenizer\n",
    "def get_tokenizer(vocabulary_size):\n",
    "  print('Training tokenizer...')\n",
    "  tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "  tweet_text = []\n",
    "  print('Read {} Sentences'.format(len(total_dataset)))\n",
    "  tokenizer.fit_on_texts(total_dataset)\n",
    "  return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OadfRX-4gU8S"
   },
   "outputs": [],
   "source": [
    "# For getting the embedding matrix\n",
    "def get_embeddings():\n",
    "  print('Generating embeddings matrix...')\n",
    "  embeddings_file = 'glove.6B.300d.txt'\n",
    "  embeddings_index = dict()\n",
    "  with open(embeddings_file, 'r', encoding=\"utf-8\") as infile:\n",
    "    for line in infile:\n",
    "      values = line.split()\n",
    "      word = values[0]\n",
    "      vector = np.asarray(values[1:], \"float32\")\n",
    "      embeddings_index[word] = vector\n",
    "\t# create a weight matrix for words in training docs\n",
    "  vocabulary_size = len(embeddings_index)\n",
    "  embeddinds_size = list(embeddings_index.values())[0].shape[0]\n",
    "  print('Vocabulary = {}, embeddings = {}'.format(vocabulary_size, embeddinds_size))\n",
    "  tokenizer = get_tokenizer(vocabulary_size)\n",
    "  embedding_matrix = np.zeros((vocabulary_size, embeddinds_size))\n",
    "  considered = 0\n",
    "  total = len(tokenizer.word_index.items())\n",
    "  for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "      print(word, index)\n",
    "      continue\n",
    "    else:\n",
    "      embedding_vector = embeddings_index.get(word)\n",
    "      if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "        considered += 1\n",
    "  print('Considered ', considered, 'Left ', total - considered)\t\t\t\n",
    "  return embedding_matrix, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "j9uPkbC-utVw"
   },
   "outputs": [],
   "source": [
    "def get_data(tokenizer, MAX_LENGTH, input_df, train=True):\n",
    "  print('Loading data')\n",
    "  X1, X2, Y = [], [], []\n",
    "  X1 = input_df['title1_en'].tolist()\n",
    "  X2 = input_df['title2_en'].tolist()\n",
    "  if train:\n",
    "    Y = input_df['label'].tolist()\n",
    "  else:\n",
    "    Y = input_df['Expected'].tolist()\n",
    "  \n",
    "  assert len(X1) == len(X2) == len(Y)\n",
    "  sequences_1 = tokenizer.texts_to_sequences(X1)\n",
    "  sequences_2 = tokenizer.texts_to_sequences(X2)\n",
    "  X1 = pad_sequences(sequences_1, maxlen=MAX_LENGTH)\n",
    "  X2 = pad_sequences(sequences_2, maxlen=MAX_LENGTH)\n",
    "  Y = np.array(Y)\n",
    "  return X1, X2, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VAga5b68yVd1",
    "outputId": "54f4cad1-1041-460f-a0f0-db19e1286c6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings matrix...\n",
      "Vocabulary = 400000, embeddings = 300\n",
      "Training tokenizer...\n",
      "Read 272705 Sentences\n",
      "Considered  32590 Left  14439\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, tokenizer = get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Gy7eZFtzcPS",
    "outputId": "9dd0d135-5594-4e57-abdf-a1f2dd365d0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 20\n",
    "# read ml data\n",
    "X1, X2, Y = get_data(tokenizer, MAX_LENGTH, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XJGcHZj47qgH",
    "outputId": "980416db-9515-44a0-bfed-762677e71759"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "X1_test, X2_test, Y_test = get_data(tokenizer, MAX_LENGTH, test_df, train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ncoMw_g7TQI",
    "outputId": "8d684ff7-9009-42d7-e37b-391d27b9e5b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320552,)\n"
     ]
    }
   ],
   "source": [
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wPvxwrgGztW5",
    "outputId": "554d863d-f1f5-42f5-fab8-87012c5b9ce7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(320552, 20)"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(X1))\n",
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2PzQhVcl0SQ0",
    "outputId": "3fed2e40-aa6a-4a46-ba74-b8a40de69ef1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " ...\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelBinarizer()#convertes into one hot form\n",
    "encoder.fit(Y)\n",
    "Y_enc = encoder.transform(Y)\n",
    "print(Y_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "r8c-velCP9XV"
   },
   "outputs": [],
   "source": [
    "# Considering everything as it is\n",
    "reduced_X1 = X1\n",
    "reduced_X2 = X2\n",
    "reduced_train_labels = train_df['label'].values\n",
    "reduced_X1_test = X1_test\n",
    "reduced_X2_test = X2_test\n",
    "reduced_test_labels = test_df['Expected'].values\n",
    "reduced_test_weights = test_df['Weight'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sJp5NKvd6xXA",
    "outputId": "381143a2-8cf0-4edc-fa1d-422e1b05246e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(320552,)\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " ...\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " ...\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(type(reduced_train_labels))\n",
    "print(reduced_train_labels.shape)\n",
    "encoder = LabelBinarizer()#convertes into one hot form\n",
    "encoder.fit(reduced_train_labels)\n",
    "Y_enc = encoder.transform(reduced_train_labels)\n",
    "Y_enc_test = encoder.transform(reduced_test_labels)\n",
    "print(Y_enc)\n",
    "print(Y_enc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UpnazOfI7g3U",
    "outputId": "ba1d6500-ebf0-468b-8fb2-06835c1e41a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "y_train = keras.utils.to_categorical(reduced_train_labels)\n",
    "print(y_train)\n",
    "y_test = keras.utils.to_categorical(reduced_test_labels)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "OwWaDbzc-e-L"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "VALIDATION_RATIO = 0.1\n",
    "RANDOM_STATE = 9527\n",
    "x1_train, x1_val, \\\n",
    "x2_train, x2_val, \\\n",
    "y_train, y_val = \\\n",
    "    train_test_split(\n",
    "        reduced_X1, reduced_X2, y_train, \n",
    "        test_size=VALIDATION_RATIO, \n",
    "        random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nxsekBsB-2J3",
    "outputId": "0589a4bc-b393-456e-8108-8dde7f32ad7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "----------\n",
      "x1_train: (288496, 20)\n",
      "x2_train: (288496, 20)\n",
      "y_train : (288496, 3)\n",
      "----------\n",
      "x1_val:   (32056, 20)\n",
      "x2_val:   (32056, 20)\n",
      "y_val :   (32056, 3)\n",
      "----------\n",
      "Test Set\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set\")\n",
    "print(\"-\" * 10)\n",
    "print(f\"x1_train: {x1_train.shape}\")\n",
    "print(f\"x2_train: {x2_train.shape}\")\n",
    "print(f\"y_train : {y_train.shape}\")\n",
    "\n",
    "print(\"-\" * 10)\n",
    "print(f\"x1_val:   {x1_val.shape}\")\n",
    "print(f\"x2_val:   {x2_val.shape}\")\n",
    "print(f\"y_val :   {y_val.shape}\")\n",
    "print(\"-\" * 10)\n",
    "print(\"Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "NowVvChI_S9Z"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 3\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 20\n",
    "\n",
    "NUM_LSTM_UNITS = 128\n",
    "\n",
    "MAX_NUM_WORDS = embedding_matrix.shape[0]\n",
    "\n",
    "NUM_EMBEDDING_DIM = embedding_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhtbldKKx6Xw",
    "outputId": "ce45b5c1-c43e-4227-fd46-8e01b5758872"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Text CNN model...\n"
     ]
    }
   ],
   "source": [
    "print('Getting Text CNN model...')\n",
    "filter_sizes = [2, 3, 5]\n",
    "num_filters = 128\t#Hyperparameters 32,64,128; 0.2,0.3,0.4\n",
    "drop = 0.4\n",
    "\n",
    "top_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "    dtype='int32')\n",
    "bm_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "    dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
    "top_embedded = embedding_layer(\n",
    "    top_input)\n",
    "bm_embedded = embedding_layer(\n",
    "    bm_input)\n",
    "reshape = Reshape((MAX_SEQUENCE_LENGTH, NUM_EMBEDDING_DIM, 1))(top_embedded)\n",
    "reshape_1 = Reshape((MAX_SEQUENCE_LENGTH, NUM_EMBEDDING_DIM, 1))(bm_embedded)\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], NUM_EMBEDDING_DIM),  padding='valid', kernel_initializer='normal',  activation='relu')(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], NUM_EMBEDDING_DIM),  padding='valid', kernel_initializer='normal',  activation='relu')(reshape_1)\n",
    "maxpool_0 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "predictions = Dense(units=NUM_CLASSES, activation='sigmoid')(dropout)\n",
    "\n",
    "model = Model(\n",
    "    inputs=[top_input, bm_input], \n",
    "    outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "vsQOKOb7zXJr"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "lr = 1e-3\n",
    "opt = Adam(lr=lr, decay=lr/50)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lfcl5Wl9yMhh",
    "outputId": "6efbafc3-8610-417f-afc4-7b7cf6a0c62d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "564/564 [==============================] - 624s 1s/step - loss: 0.5713 - accuracy: 0.7418 - val_loss: 0.4974 - val_accuracy: 0.7894\n",
      "Epoch 2/50\n",
      "564/564 [==============================] - 635s 1s/step - loss: 0.4394 - accuracy: 0.8091 - val_loss: 0.4802 - val_accuracy: 0.7998\n",
      "Epoch 3/50\n",
      "564/564 [==============================] - 627s 1s/step - loss: 0.3952 - accuracy: 0.8290 - val_loss: 0.4751 - val_accuracy: 0.8052\n",
      "Epoch 4/50\n",
      "564/564 [==============================] - 609s 1s/step - loss: 0.3700 - accuracy: 0.8392 - val_loss: 0.4870 - val_accuracy: 0.8030\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 50\n",
    "stop = [EarlyStopping(monitor='val_loss', patience=0.001)]\n",
    "history = model.fit(x=[x1_train, x1_train],\n",
    "                    y=y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=(\n",
    "                      [x1_val, x2_val], \n",
    "                      y_val\n",
    "                    ),\n",
    "                    shuffle=True,\n",
    "                    callbacks=stop,\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "qKCup0UbzfTx"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "predictions = model.predict(\n",
    "    [reduced_X1_test, reduced_X2_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_OilvEd4zlbJ",
    "outputId": "da005565-64f1-432f-a6cb-667f6c60bc32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is\n",
      "65.8927441703068\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      agreed       0.67      0.34      0.45 1805.000009024593\n",
      "   disagreed       0.40      0.01      0.01 334.1999999999896\n",
      "   unrelated       0.66      0.91      0.76   3211.25\n",
      "\n",
      "    accuracy                           0.66 5350.450009024583\n",
      "   macro avg       0.57      0.42      0.41 5350.450009024583\n",
      "weighted avg       0.64      0.66      0.61 5350.450009024583\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = [idx for idx in np.argmax(predictions, axis=1)]\n",
    "#print(y_pred)\n",
    "print('Accuracy is')\n",
    "print(metrics.accuracy_score(reduced_test_labels, y_pred, sample_weight = reduced_test_weights)*100)\n",
    "print(classification_report(reduced_test_labels, y_pred, target_names = ['agreed', 'disagreed', 'unrelated'], sample_weight = reduced_test_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jRsGiCux_3YA",
    "outputId": "f11f1835-8bf6-4598-e45f-9a259f4ae362"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 20, 300)      120000000   input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 128)          219648      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           lstm[0][0]                       \n",
      "                                                                 lstm[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            771         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 120,220,419\n",
      "Trainable params: 120,220,419\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "top_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "    dtype='int32')\n",
    "bm_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "    dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
    "top_embedded = embedding_layer(\n",
    "    top_input)\n",
    "bm_embedded = embedding_layer(\n",
    "    bm_input)\n",
    "\n",
    "shared_lstm = LSTM(NUM_LSTM_UNITS)\n",
    "top_output = shared_lstm(top_embedded)\n",
    "bm_output = shared_lstm(bm_embedded)\n",
    "\n",
    "merged = concatenate(\n",
    "    [top_output, bm_output], \n",
    "    axis=-1)\n",
    "\n",
    "dense =  Dense(\n",
    "    units=NUM_CLASSES, \n",
    "    activation='softmax')\n",
    "predictions = dense(merged)\n",
    "\n",
    "model = Model(\n",
    "    inputs=[top_input, bm_input], \n",
    "    outputs=predictions)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "U9Y3pD2sA5YP"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "lr = 1e-3\n",
    "opt = Adam(lr=lr, decay=lr/50)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JuxjGECRBbP3",
    "outputId": "53f6c516-be6b-4bb7-a1f3-a6e60f8aef6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "564/564 [==============================] - 619s 1s/step - loss: 0.4963 - accuracy: 0.7746 - val_loss: 0.4181 - val_accuracy: 0.8106\n",
      "Epoch 2/50\n",
      "564/564 [==============================] - 610s 1s/step - loss: 0.3591 - accuracy: 0.8416 - val_loss: 0.3911 - val_accuracy: 0.8283\n",
      "Epoch 3/50\n",
      "564/564 [==============================] - 633s 1s/step - loss: 0.3014 - accuracy: 0.8684 - val_loss: 0.3951 - val_accuracy: 0.8345\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 50\n",
    "stop = [EarlyStopping(monitor='val_loss', patience=0.001)]\n",
    "history = model.fit(x=[x1_train, x2_train],\n",
    "                    y=y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=(\n",
    "                      [x1_val, x2_val], \n",
    "                      y_val\n",
    "                    ),\n",
    "                    shuffle=True,\n",
    "                    callbacks=[stop],\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "I6zQysQjEAAt"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "predictions = model.predict(\n",
    "    [reduced_X1_test, reduced_X2_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LVcCnCN9I1Rt",
    "outputId": "54f1b948-1a78-402c-ca1c-c38020a17480"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is\n",
      "66.71853142406066\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      agreed       0.64      0.44      0.52 1805.000009024593\n",
      "   disagreed       0.62      0.09      0.16 334.1999999999896\n",
      "   unrelated       0.68      0.86      0.76   3211.25\n",
      "\n",
      "    accuracy                           0.67 5350.450009024583\n",
      "   macro avg       0.65      0.46      0.48 5350.450009024583\n",
      "weighted avg       0.66      0.67      0.64 5350.450009024583\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = [idx for idx in np.argmax(predictions, axis=1)]\n",
    "#print(y_pred)\n",
    "print('Accuracy is')\n",
    "print(metrics.accuracy_score(reduced_test_labels, y_pred, sample_weight = reduced_test_weights)*100)\n",
    "print(classification_report(reduced_test_labels, y_pred, target_names = ['agreed', 'disagreed', 'unrelated'], sample_weight = reduced_test_weights))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BD_Baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
