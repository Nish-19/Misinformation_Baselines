{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EQBT609POS5u",
    "outputId": "e2ee146a-d8ae-46d8-c9bd-7f5a2067b56e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-Paueg_JOUI-"
   },
   "outputs": [],
   "source": [
    "# All general imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, Reshape, Conv2D, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Bidirectional, GlobalAveragePooling1D, GRU, GlobalMaxPooling1D, concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LSTM, GRU, Conv1D, MaxPool1D, Activation\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Conv1D, Softmax\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import io, os, gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "id": "hwuJb4mwOWzf",
    "outputId": "13eed8f0-8a21-4f77-c88f-a14e5e771209"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'text', 'stance', 'Novelty_Quora', 'Emotion_Label',\n",
      "       'com_femotion'],\n",
      "      dtype='object')\n",
      "Index(['id', 'text', 'stance', 'Novelty_Quora', 'Emotion_Label',\n",
      "       'com_femotion'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>stance</th>\n",
       "      <th>Novelty_Quora</th>\n",
       "      <th>Emotion_Label</th>\n",
       "      <th>com_femotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.250000e+18</td>\n",
       "      <td>what is the reason the cdc is hiding info re h...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.250000e+18</td>\n",
       "      <td>so you are denying that dr vladimir zelenko is...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.250000e+18</td>\n",
       "      <td>anyone whos been to a malaria area would in th...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.260000e+18</td>\n",
       "      <td>many countries preceded us in adopting chloroq...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.260000e+18</td>\n",
       "      <td>those who proselytize against hydroxychloroqui...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  ... com_femotion\n",
       "0  1.250000e+18  ...            1\n",
       "1  1.250000e+18  ...            1\n",
       "2  1.250000e+18  ...            1\n",
       "3  1.260000e+18  ...            1\n",
       "4  1.260000e+18  ...            1\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################### Importing ByteDance Datasets ####################\n",
    "# Train set\n",
    "train_df = pd.read_csv('../Datasets/cstance_train.csv')\n",
    "print(train_df.columns)\n",
    "train_df.head()\n",
    "\n",
    "# Test set\n",
    "test_df = pd.read_csv('../Datasets/cstance_test_new.csv')\n",
    "print(test_df.columns)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X9Z02op3Ovxi",
    "outputId": "e19b6f79-eaeb-4bfb-cf82-a3708d33eef9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise (8572, 768)\n",
      "Hypothesis (8572, 768)\n"
     ]
    }
   ],
   "source": [
    "pre_bert_cs = np.load(\"../Datasets/pre_bert_accs.npy\")\n",
    "hyp_bert_cs = np.load(\"../Datasets/hyp_bert_accs.npy\")\n",
    "print('Premise', pre_bert_cs.shape)\n",
    "print('Hypothesis', hyp_bert_cs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IlCRC2q4OzJA",
    "outputId": "54704264-c9d4-4b58-9151-bbede0067e74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise (2494, 768)\n",
      "Hypothesis (2494, 768)\n"
     ]
    }
   ],
   "source": [
    "pre_bert_cs_test = np.load(\"../Datasets/pre_bert_test_accs.npy\")\n",
    "hyp_bert_cs_test = np.load(\"../Datasets/hyp_bert_test_accs.npy\")\n",
    "print('Premise', pre_bert_cs_test.shape)\n",
    "print('Hypothesis', hyp_bert_cs_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mmJMwVB4OYPG",
    "outputId": "9b11c815-214d-4b49-dad8-1d17ff463a83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8572\n",
      "7185\n",
      "Train Length is 7186\n",
      "Test merged 2366\n",
      "Dataset length is 9552\n"
     ]
    }
   ],
   "source": [
    "premise = [\"chloroquine hydroxychloroquine are cure for the novel coronavirus\"]\n",
    "train_lst_1 = train_df['text'].tolist()\n",
    "print(len(train_lst_1))\n",
    "train_lst_1[:5]\n",
    "uq_tr_1 = list(set(train_lst_1))\n",
    "print(len(uq_tr_1))\n",
    "train_merged = uq_tr_1 + premise\n",
    "print('Train Length is', len(train_merged))\n",
    "train_merged[:5]\n",
    "test_lst_1 = test_df['text'].tolist()\n",
    "uq_ts_1 = list(set(test_lst_1))\n",
    "test_merged = uq_ts_1\n",
    "print('Test merged', len(test_merged))\n",
    "total_dataset = train_merged + test_merged\n",
    "print('Dataset length is', len(total_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cPFtXkdtOZve"
   },
   "outputs": [],
   "source": [
    "# Defining the tokenizer\n",
    "def get_tokenizer(vocabulary_size):\n",
    "  print('Training tokenizer...')\n",
    "  tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "  tweet_text = []\n",
    "  print('Read {} Sentences'.format(len(total_dataset)))\n",
    "  tokenizer.fit_on_texts(total_dataset)\n",
    "  return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mw7ZPFZkObE2"
   },
   "outputs": [],
   "source": [
    "# For getting the embedding matrix\n",
    "def get_embeddings():\n",
    "  print('Generating embeddings matrix...')\n",
    "  embeddings_file = 'glove.6B.300d.txt'\n",
    "  embeddings_index = dict()\n",
    "  with open(embeddings_file, 'r', encoding=\"utf-8\") as infile:\n",
    "    for line in infile:\n",
    "      values = line.split()\n",
    "      word = values[0]\n",
    "      vector = np.asarray(values[1:], \"float32\")\n",
    "      embeddings_index[word] = vector\n",
    "\t# create a weight matrix for words in training docs\n",
    "  vocabulary_size = len(embeddings_index)\n",
    "  embeddinds_size = list(embeddings_index.values())[0].shape[0]\n",
    "  print('Vocabulary = {}, embeddings = {}'.format(vocabulary_size, embeddinds_size))\n",
    "  tokenizer = get_tokenizer(vocabulary_size)\n",
    "  embedding_matrix = np.zeros((vocabulary_size, embeddinds_size))\n",
    "  considered = 0\n",
    "  total = len(tokenizer.word_index.items())\n",
    "  for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "      print(word, index)\n",
    "      continue\n",
    "    else:\n",
    "      embedding_vector = embeddings_index.get(word)\n",
    "      if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "        considered += 1\n",
    "  print('Considered ', considered, 'Left ', total - considered)\t\t\t\n",
    "  return embedding_matrix, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4uaKjxBBOccG"
   },
   "outputs": [],
   "source": [
    "def get_data(tokenizer, MAX_LENGTH, input_df):\n",
    "  print('Loading data')\n",
    "  X1, X2, Y = [], [], []\n",
    "  X2 = input_df['text'].tolist()\n",
    "  length = len(X2)\n",
    "  premise = \"chloroquine hydroxychloroquine are cure for the novel coronavirus\"\n",
    "  X1 = [premise for i in range(length)]\n",
    "  Y = input_df['stance'].tolist()\n",
    "  new_Y = [(ele-1) for ele in Y]\n",
    "  assert len(new_Y) == len(Y)\n",
    "  \n",
    "  len(X1) == len(X2) == len(Y)\n",
    "  sequences_1 = tokenizer.texts_to_sequences(X1)\n",
    "  sequences_2 = tokenizer.texts_to_sequences(X2)\n",
    "\t# for i, s in enumerate(sequences):\n",
    "\t# \tsequences[i] = sequences[i][-250:]\n",
    "  X1 = pad_sequences(sequences_1, maxlen=MAX_LENGTH)\n",
    "  X2 = pad_sequences(sequences_2, maxlen=MAX_LENGTH)\n",
    "  new_Y = np.array(new_Y)\n",
    "  return X1, X2, new_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MDb4NBGXOeEW",
    "outputId": "22c9aefb-5382-4b2a-98da-42dfe4637bb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings matrix...\n",
      "Vocabulary = 400000, embeddings = 300\n",
      "Training tokenizer...\n",
      "Read 9552 Sentences\n",
      "Considered  11908 Left  4553\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, tokenizer = get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "swyJLZiqOfrn",
    "outputId": "27778fec-0264-4f04-8462-226f09005c65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 50\n",
    "# read ml data\n",
    "X1, X2, Y = get_data(tokenizer, MAX_LENGTH, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SN9RC3Q1OhCf",
    "outputId": "caf4bcd7-00e3-40f7-964d-d68783044d30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "X1_test, X2_test, Y_test = get_data(tokenizer, MAX_LENGTH, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0gnF4-mnOiW-",
    "outputId": "3fdf7440-bd92-4a86-99b4-344f6590edf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelBinarizer()#convertes into one hot form\n",
    "encoder.fit(Y)\n",
    "Y_enc = encoder.transform(Y)\n",
    "Y_enc_test = encoder.transform(Y_test)\n",
    "print(Y_enc)\n",
    "print(Y_enc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ns5_B4BoOl92",
    "outputId": "c8e698fa-a88c-4a52-f5c3-94b61ad8ab24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "y_train = keras.utils.to_categorical(Y)\n",
    "print(y_train)\n",
    "y_test = keras.utils.to_categorical(Y_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "bCpT1s_XOmVO"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "VALIDATION_RATIO = 0.1\n",
    "RANDOM_STATE = 9527\n",
    "x1_train, x1_val, \\\n",
    "x2_train, x2_val, \\\n",
    "pre_train_bert, pre_val_bert, \\\n",
    "hyp_train_bert, hyp_val_bert, \\\n",
    "y_train, y_val = \\\n",
    "    train_test_split(\n",
    "        X1, X2, pre_bert_cs, hyp_bert_cs,\n",
    "        y_train,\n",
    "        test_size=VALIDATION_RATIO, \n",
    "        random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nVDhuab5OnyG",
    "outputId": "a29e8863-5c0e-4feb-fe3d-7f63bb409757"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "----------\n",
      "x1_train: (7714, 50)\n",
      "x2_train: (7714, 50)\n",
      "y_train : (7714, 2)\n",
      "Bert premise : (7714, 768)\n",
      "----------\n",
      "x1_val:   (858, 50)\n",
      "x2_val:   (858, 50)\n",
      "y_val :   (858, 2)\n",
      "Val_Bert_premise : (858, 768)\n",
      "----------\n",
      "Test Set\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set\")\n",
    "print(\"-\" * 10)\n",
    "print(f\"x1_train: {x1_train.shape}\")\n",
    "print(f\"x2_train: {x2_train.shape}\")\n",
    "print(f\"y_train : {y_train.shape}\")\n",
    "print(f\"Bert premise : {pre_train_bert.shape}\")\n",
    "\n",
    "print(\"-\" * 10)\n",
    "print(f\"x1_val:   {x1_val.shape}\")\n",
    "print(f\"x2_val:   {x2_val.shape}\")\n",
    "print(f\"y_val :   {y_val.shape}\")\n",
    "print(f\"Val_Bert_premise : {pre_val_bert.shape}\")\n",
    "print(\"-\" * 10)\n",
    "print(\"Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "NCK9cdYiOpEH"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "\n",
    "NUM_LSTM_UNITS = 150\n",
    "\n",
    "MAX_NUM_WORDS = embedding_matrix.shape[0]\n",
    "\n",
    "NUM_EMBEDDING_DIM = embedding_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hj9_IxZyPNuu",
    "outputId": "4944acf3-ce6f-4b59-aa24-7b0133736b09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 768)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 768)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 300)      120000000   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 1, 768)       0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 768)       0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 50, 300)      541200      embedding[0][0]                  \n",
      "                                                                 embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1, 300)       1102800     reshape[0][0]                    \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 50, 600)      0           bidirectional[0][0]              \n",
      "                                                                 bidirectional[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 600)       0           bidirectional_2[0][0]            \n",
      "                                                                 bidirectional_2[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 300)          901200      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 300)          901200      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2 (TensorFlowOp [(None, 300)]        0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub (TensorFlowOpLa [(None, 300)]        0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul (TensorFlowOpLa [(None, 300)]        0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 900)          0           tf_op_layer_AddV2[0][0]          \n",
      "                                                                 tf_op_layer_Sub[0][0]            \n",
      "                                                                 tf_op_layer_Mul[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "pre_fnc (Dense)                 (None, 64)           57664       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "fnc (Dense)                     (None, 2)            130         pre_fnc[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 123,504,194\n",
      "Trainable params: 123,504,194\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# BERT + Normal Grand Model\n",
    "\n",
    "NUM_LSTM_UNITS = 150\n",
    "\n",
    "top_input_wd = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "    dtype='int32')\n",
    "bm_input_wd = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "    dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
    "top_embedded_wd = embedding_layer(\n",
    "    top_input_wd)\n",
    "bm_embedded_wd = embedding_layer(\n",
    "    bm_input_wd)\n",
    "\n",
    "source_lstm_wd = Bidirectional(LSTM(NUM_LSTM_UNITS, return_sequences=True, recurrent_dropout = 0.3))\n",
    "shared_lstm_wd = Bidirectional(LSTM(NUM_LSTM_UNITS, activation='tanh', recurrent_dropout = 0.3))\n",
    "top_source_wd = source_lstm_wd(top_embedded_wd)\n",
    "bm_source_wd = source_lstm_wd(bm_embedded_wd)\n",
    "\n",
    "source_comb_wd = concatenate(\n",
    "    [top_source_wd, bm_source_wd],\n",
    "    axis=-1\n",
    "    )\n",
    "lstm_ops_wd = shared_lstm_wd(source_comb_wd)   # 300D vector\n",
    "\n",
    "\n",
    "top_input_bt = Input(\n",
    "    shape=(768, ), \n",
    "    dtype='float32')\n",
    "bm_input_bt = Input(\n",
    "    shape=(768, ), \n",
    "    dtype='float32')\n",
    "\n",
    "\n",
    "top_embedded_bt = Reshape((1, 768, ))(top_input_bt)\n",
    "bm_embedded_bt = Reshape((1, 768, ))(bm_input_bt)\n",
    "\n",
    "source_lstm_bt = Bidirectional(LSTM(NUM_LSTM_UNITS, return_sequences=True, recurrent_dropout = 0.3))\n",
    "shared_lstm_bt = Bidirectional(LSTM(NUM_LSTM_UNITS, activation='tanh', recurrent_dropout = 0.3))\n",
    "top_source_bt = source_lstm_bt(top_embedded_bt)\n",
    "bm_source_bt = source_lstm_bt(bm_embedded_bt)\n",
    "\n",
    "source_comb_bt = concatenate(\n",
    "    [top_source_bt, bm_source_bt],\n",
    "    axis=-1\n",
    "    )\n",
    "lstm_ops_bt = shared_lstm_bt(source_comb_bt)  #300D vector\n",
    "\n",
    "# Bert and Normal Combination\n",
    "\n",
    "comb_features_cs = concatenate(\n",
    "    [lstm_ops_wd+lstm_ops_bt, lstm_ops_wd-lstm_ops_bt, lstm_ops_wd*lstm_ops_bt],\n",
    "    axis=-1\n",
    "    )\n",
    "\n",
    "pre_cs = Dense(\n",
    "    units=64, \n",
    "    activation='tanh',\n",
    "    name = 'pre_fnc')(comb_features_cs)\n",
    "\n",
    "dense_cs =  Dense(\n",
    "    units=NUM_CLASSES, \n",
    "    activation='softmax',\n",
    "    name = 'fnc')\n",
    "\n",
    "predictions_cs = dense_cs(pre_cs)\n",
    "\n",
    "model = Model(\n",
    "    inputs=[top_input_wd, bm_input_wd, top_input_bt, bm_input_bt], \n",
    "    outputs=[predictions_cs])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "kOKRRJA2Pvpn"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "lr = 1e-3\n",
    "opt = Adam(lr=lr, decay=lr/50)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2_9Kp230PyoC",
    "outputId": "e169762b-b2f8-4d33-c89a-57c2384c3649"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "16/16 [==============================] - 32s 2s/step - loss: 0.6448 - accuracy: 0.6178 - val_loss: 0.5020 - val_accuracy: 0.7308\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 0.3076 - accuracy: 0.8691 - val_loss: 0.2996 - val_accuracy: 0.8683\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 30s 2s/step - loss: 0.1305 - accuracy: 0.9480 - val_loss: 0.3229 - val_accuracy: 0.8869\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 50\n",
    "stop = [EarlyStopping(monitor='val_loss', patience=0.001)]\n",
    "history = model.fit(x=[x1_train, x2_train, pre_train_bert, hyp_train_bert],\n",
    "                    y=y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=(\n",
    "                      [x1_val, x2_val, pre_val_bert, hyp_val_bert], \n",
    "                      y_val\n",
    "                    ),\n",
    "                    shuffle=True,\n",
    "                    callbacks=stop,\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "_57cce8GRMc_"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "predictions = model.predict(\n",
    "    [X1_test, X2_test, pre_bert_cs_test, hyp_bert_cs_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MkMuBUm2RRkg",
    "outputId": "241e6b66-a98d-4414-c2ff-6f32187b7ed4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is\n",
      "83.96150761828387\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     against       0.80      0.80      0.80      1007\n",
      "         for       0.86      0.87      0.87      1487\n",
      "\n",
      "    accuracy                           0.84      2494\n",
      "   macro avg       0.83      0.83      0.83      2494\n",
      "weighted avg       0.84      0.84      0.84      2494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = [idx for idx in np.argmax(predictions, axis=1)]\n",
    "test_labels = test_df['stance'].tolist()\n",
    "n_test_labels = [(ele-1) for ele in test_labels]\n",
    "print('Accuracy is')\n",
    "print(metrics.accuracy_score(n_test_labels, y_pred)*100)\n",
    "print(classification_report(n_test_labels, y_pred, target_names = ['against', 'for']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jibrFgNKRfDr",
    "outputId": "f885f42f-e968-4606-8039-17e94330aecd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Text CNN model...\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 50, 300)      120000000   input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 50, 300, 1)   0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 50, 300, 1)   0           embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 49, 1, 128)   76928       reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 48, 1, 128)   115328      reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 768)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 768)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 1, 1, 128)    0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 128)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 1, 768)       0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 1, 768)       0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 2, 1, 128)    0           max_pooling2d[0][0]              \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1, 256)       918528      reshape_4[0][0]                  \n",
      "                                                                 reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 256)          0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1, 512)       0           bidirectional_4[0][0]            \n",
      "                                                                 bidirectional_4[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 256)          656384      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_1 (TensorFlow [(None, 256)]        0           dropout[0][0]                    \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_1 (TensorFlowOp [(None, 256)]        0           dropout[0][0]                    \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_1 (TensorFlowOp [(None, 256)]        0           dropout[0][0]                    \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 768)          0           tf_op_layer_AddV2_1[0][0]        \n",
      "                                                                 tf_op_layer_Sub_1[0][0]          \n",
      "                                                                 tf_op_layer_Mul_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2)            1538        concatenate_5[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 121,768,706\n",
      "Trainable params: 121,768,706\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model 2\n",
    "NUM_LSTM_UNITS = 128\n",
    "\n",
    "print('Getting Text CNN model...')\n",
    "filter_sizes = [2, 3, 5]\n",
    "num_filters = 128\t#Hyperparameters 32,64,128; 0.2,0.3,0.4\n",
    "drop = 0.4\n",
    "\n",
    "top_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "    dtype='int32')\n",
    "bm_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "    dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
    "top_embedded = embedding_layer(\n",
    "    top_input)\n",
    "bm_embedded = embedding_layer(\n",
    "    bm_input)\n",
    "reshape = Reshape((MAX_SEQUENCE_LENGTH, NUM_EMBEDDING_DIM, 1))(top_embedded)\n",
    "reshape_1 = Reshape((MAX_SEQUENCE_LENGTH, NUM_EMBEDDING_DIM, 1))(bm_embedded)\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], NUM_EMBEDDING_DIM),  padding='valid', kernel_initializer='normal',  activation='relu')(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], NUM_EMBEDDING_DIM),  padding='valid', kernel_initializer='normal',  activation='relu')(reshape_1)\n",
    "maxpool_0 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "\n",
    "top_input_bt = Input(\n",
    "    shape=(768, ), \n",
    "    dtype='float32')\n",
    "bm_input_bt = Input(\n",
    "    shape=(768, ), \n",
    "    dtype='float32')\n",
    "\n",
    "\n",
    "top_embedded_bt = Reshape((1, 768, ))(top_input_bt)\n",
    "bm_embedded_bt = Reshape((1, 768, ))(bm_input_bt)\n",
    "\n",
    "source_lstm_bt = Bidirectional(LSTM(NUM_LSTM_UNITS, return_sequences=True, recurrent_dropout = 0.3))\n",
    "shared_lstm_bt = Bidirectional(LSTM(NUM_LSTM_UNITS, activation='tanh', recurrent_dropout = 0.3))\n",
    "top_source_bt = source_lstm_bt(top_embedded_bt)\n",
    "bm_source_bt = source_lstm_bt(bm_embedded_bt)\n",
    "\n",
    "source_comb_bt = concatenate(\n",
    "    [top_source_bt, bm_source_bt],\n",
    "    axis=-1\n",
    "    )\n",
    "lstm_ops_bt = shared_lstm_bt(source_comb_bt)  #256D vector\n",
    "\n",
    "comb_features_cs = concatenate(\n",
    "    [dropout+lstm_ops_bt, dropout-lstm_ops_bt, dropout*lstm_ops_bt],\n",
    "    axis=-1\n",
    "    )\n",
    "\n",
    "predictions = Dense(units=NUM_CLASSES, activation='sigmoid')(comb_features_cs)\n",
    "\n",
    "model = Model(\n",
    "    inputs=[top_input, bm_input, top_input_bt, bm_input_bt], \n",
    "    outputs=predictions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "msvsG7PERkl7"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "lr = 1e-3\n",
    "opt = Adam(lr=lr, decay=lr/50)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MTI7wQQsRoT6",
    "outputId": "575b9cf7-2fc7-4192-b9fb-951b8c94c650"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "16/16 [==============================] - 18s 1s/step - loss: 0.6435 - accuracy: 0.6123 - val_loss: 0.5768 - val_accuracy: 0.6888\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 17s 1s/step - loss: 0.4794 - accuracy: 0.7721 - val_loss: 0.4098 - val_accuracy: 0.8007\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 17s 1s/step - loss: 0.3351 - accuracy: 0.8616 - val_loss: 0.3288 - val_accuracy: 0.8508\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 17s 1s/step - loss: 0.2211 - accuracy: 0.9213 - val_loss: 0.3012 - val_accuracy: 0.8648\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 17s 1s/step - loss: 0.1467 - accuracy: 0.9542 - val_loss: 0.2752 - val_accuracy: 0.8811\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 17s 1s/step - loss: 0.0914 - accuracy: 0.9739 - val_loss: 0.2826 - val_accuracy: 0.8718\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 50\n",
    "stop = [EarlyStopping(monitor='val_loss', patience=0.001)]\n",
    "history = model.fit(x=[x1_train, x2_train, pre_train_bert, hyp_train_bert],\n",
    "                    y=y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=(\n",
    "                      [x1_val, x2_val, pre_val_bert, hyp_val_bert], \n",
    "                      y_val\n",
    "                    ),\n",
    "                    shuffle=True,\n",
    "                    callbacks=stop,\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "69-VM13xRtgw"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "predictions = model.predict(\n",
    "    [X1_test, X2_test, pre_bert_cs_test, hyp_bert_cs_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Z3y5YKgRwD7",
    "outputId": "b77b6471-6f0f-458d-c497-e8b87125a600"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is\n",
      "85.44506816359262\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     against       0.82      0.82      0.82      1007\n",
      "         for       0.88      0.88      0.88      1487\n",
      "\n",
      "    accuracy                           0.85      2494\n",
      "   macro avg       0.85      0.85      0.85      2494\n",
      "weighted avg       0.85      0.85      0.85      2494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = [idx for idx in np.argmax(predictions, axis=1)]\n",
    "test_labels = test_df['stance'].tolist()\n",
    "n_test_labels = [(ele-1) for ele in test_labels]\n",
    "print('Accuracy is')\n",
    "print(metrics.accuracy_score(n_test_labels, y_pred)*100)\n",
    "print(classification_report(n_test_labels, y_pred, target_names = ['against', 'for']))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CS_Bert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
